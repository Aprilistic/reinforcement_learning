{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def step(self, a):\n",
    "        if a == 0:\n",
    "            self.move_right()\n",
    "        elif a == 1:\n",
    "            self.move_left()\n",
    "        elif a == 2:\n",
    "            self.move_up()\n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "\n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def move_right(self):\n",
    "        self.y += 1\n",
    "        if self.y > 3:\n",
    "            self.y = 3\n",
    "\n",
    "    def move_left(self):\n",
    "        self.y -= 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "\n",
    "    def move_up(self):\n",
    "        self.x -= 1\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "\n",
    "    def move_down(self):\n",
    "        self.x += 1\n",
    "        if self.x > 3:\n",
    "            self.x = 3\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.x == 3 and self.y == 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self):\n",
    "        coin = random.random()\n",
    "        if coin < 0.25:\n",
    "            action = 0\n",
    "        elif coin < 0.5:\n",
    "            action = 1\n",
    "        elif coin < 0.75:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 3\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_evaluation(gamma: float = 1.0, alpha: float = 0.001, num_episodes: int = 50000):\n",
    "    env = GridWorld()\n",
    "    agent = Agent()\n",
    "    V = np.zeros((4, 4), dtype=float)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "        terminated = False\n",
    "        trajectory = []\n",
    "        while not terminated:\n",
    "            action = agent.select_action()\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            trajectory.append((state, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for transition in trajectory[::-1]:\n",
    "            state, reward = transition\n",
    "            G = reward + gamma*G\n",
    "            V[state] = V[state] + alpha*(G - V[state])\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Step_TD_evaluation(gamma: float = 1.0, alpha: float = 0.001, num_episodes: int = 50000):\n",
    "    env = GridWorld()\n",
    "    agent = Agent()\n",
    "    V = np.zeros((4, 4), dtype=float)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            action = agent.select_action()\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            V[state] = V[state] + alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            state = next_state\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-step TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Step_TD_evaluation(n: int, gamma: float = 1.0, alpha: float = 0.001, num_episodes: int = 50000):\n",
    "    env = GridWorld()\n",
    "    agent = Agent()\n",
    "    V = np.zeros((4, 4), dtype=float)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        env.reset()\n",
    "        state = env.get_state()\n",
    "        terminated = False\n",
    "        trajectory = []\n",
    "        trajectory.append((state, 0))\n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action = agent.select_action()\n",
    "                next_state, reward, terminated = env.step(action)\n",
    "                trajectory.append((state, reward))\n",
    "                \n",
    "                if terminated:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    state = next_state\n",
    "                \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n + 1, T + 1)):\n",
    "                    G += np.power(gamma, i - tau - 1) * trajectory[i - 1][1]\n",
    "                if tau + n < T:\n",
    "                    G += np.power(gamma, n) * V[trajectory[tau + n][0]]\n",
    "                V[trajectory[tau][0]] += alpha * (G - V[trajectory[tau][0]])\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "            \n",
    "            t += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:02<00:00, 23713.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC: [[-58.83863793 -58.76293453 -57.47678121 -55.27356081]\n",
      " [-58.49380835 -55.36528549 -51.59867416 -46.9189773 ]\n",
      " [-54.14887321 -51.70238293 -43.69344589 -31.94178156]\n",
      " [-50.50331016 -45.00110269 -30.35996954   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:02<00:00, 20909.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0): [[-58.24218197 -56.2696774  -53.21787154 -50.6497432 ]\n",
      " [-56.33610727 -53.49983492 -48.69766837 -43.98591504]\n",
      " [-53.38461657 -48.84019297 -39.62675312 -29.38989848]\n",
      " [-50.85609321 -44.35489245 -29.18641246   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:11<00:00, 4201.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-step TD: [[-58.97615095 -57.16265775 -54.04242489 -51.47193741]\n",
      " [-57.07993237 -54.27139759 -49.34511961 -44.68002261]\n",
      " [-54.19273035 -49.59521942 -41.09070014 -39.96876671]\n",
      " [-51.1754361  -45.15644746 -38.86775757   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:14<00:00, 3414.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-step TD: [[-58.76535374 -56.67811873 -53.76951313 -51.23389203]\n",
      " [-56.87954977 -54.10646931 -48.79871629 -44.5217288 ]\n",
      " [-53.65931363 -48.98768838 -40.11770205 -38.48560602]\n",
      " [-51.61010445 -45.18975118 -39.29081079   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:17<00:00, 2875.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-step TD: [[-58.43711717 -55.95447468 -52.56769571 -49.82503799]\n",
      " [-56.13261163 -52.87183063 -47.97055328 -42.92139439]\n",
      " [-53.44159211 -48.65658471 -39.33854354 -37.82579109]\n",
      " [-51.23957351 -43.94466911 -37.97164198   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:19<00:00, 2506.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-step TD: [[-57.64545814 -55.6408881  -53.07337495 -50.0285047 ]\n",
      " [-56.0469075  -52.57359134 -48.17039731 -44.0588493 ]\n",
      " [-53.03198438 -47.68805735 -38.91878643 -38.43445102]\n",
      " [-50.41189012 -42.2798974  -37.23235966   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:32<00:00, 1546.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-step TD: [[-60.05326182 -57.4514421  -54.30990279 -51.38334077]\n",
      " [-58.12224859 -54.70367833 -50.60562422 -46.37460811]\n",
      " [-54.74309817 -49.97377345 -41.59657487 -39.83903496]\n",
      " [-51.53638691 -44.81786411 -39.16800497   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:53<00:00, 935.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-step TD: [[-59.19236005 -57.30449697 -54.23283098 -50.41387899]\n",
      " [-56.67984293 -54.43201054 -50.32083253 -45.18343911]\n",
      " [-54.91599469 -50.58953781 -42.45278647 -40.480507  ]\n",
      " [-52.81282423 -48.36942643 -42.16864308   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"MC:\", MC_evaluation())\n",
    "print(\"TD(0):\", One_Step_TD_evaluation())\n",
    "for n in [2, 3, 4, 5, 10, 20]:\n",
    "    print(f\"{n}-step TD:\", N_Step_TD_evaluation(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
