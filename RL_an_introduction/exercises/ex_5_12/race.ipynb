{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Generator:\n",
    "    def widen_hole_transformation(self, racetrack, start_cell, end_cell):\n",
    "        δ = 1\n",
    "        while 1:\n",
    "            if ((start_cell[1] < δ) or (start_cell[0] < δ)):\n",
    "                racetrack[0:end_cell[0], 0:end_cell[1]] = 1\n",
    "                break\n",
    "\n",
    "            if ((end_cell[1] > 100 - δ) or (end_cell[0] > 100 - δ)):\n",
    "                racetrack[start_cell[0]:100, start_cell[1]:100] = 1\n",
    "                break\n",
    "\n",
    "            δ += 1\n",
    "\n",
    "        return racetrack\n",
    "    \n",
    "    def calculate_valid_fraction(self, racetrack):\n",
    "        return len(racetrack[racetrack == 0]) / 10000\n",
    "    \n",
    "    def mark_finish_states(self, racetrack):\n",
    "        last_row = racetrack[0, 0:100]\n",
    "        last_row[last_row == 0] = 2\n",
    "        return racetrack\n",
    "    \n",
    "    def mark_start_states(self, racetrack):\n",
    "        last_row = racetrack[99, 0:100]\n",
    "        last_row[last_row == 0] = 1\n",
    "        return racetrack\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def generate_racetrack(self):\n",
    "        racetrack = np.zeros((100, 100), dtype='int')\n",
    "\n",
    "        frac = 1\n",
    "        while frac > 0.5:\n",
    "            random_cell = np.random.randint((100, 100))\n",
    "            random_hole_dims = np.random.randint((25, 25))\n",
    "            start_cell = np.array([max(0, x - y//2) for x, y in zip(random_cell, random_hole_dims)])\n",
    "            end_cell = np.array([min(100, x + y) for x, y in zip(random_cell, random_hole_dims)])\n",
    "\n",
    "            racetrack = self.widen_hole_transformation(racetrack, start_cell, end_cell)\n",
    "            frac = self.calculate_valid_fraction(racetrack)\n",
    "\n",
    "        racetrack = self.mark_start_states(racetrack)\n",
    "        racetrack = self.mark_finish_states(racetrack)\n",
    "\n",
    "        return racetrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def get_new_state(self, state, action):\n",
    "        new_state = state.copy()\n",
    "        new_state[0] = state[0] - state[2]\n",
    "        new_state[1] = state[1] + state[3]\n",
    "        new_state[2] = state[2] + action[0]\n",
    "        new_state[3] = state[3] + action[1]\n",
    "        return new_state\n",
    "    \n",
    "    def select_randomly(self, NUMPY_ARR):\n",
    "        return np.random.choice(NUMPY_ARR)\n",
    "    \n",
    "    def set_zero(NUMPY_ARR):\n",
    "        NUMPY_ARR[:] = 0\n",
    "        return NUMPY_ARR\n",
    "    \n",
    "    def is_finish_line_crossed(self, state, action):\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "        rows = np.array(range(new_cell[0], old_cell[0] + 1))\n",
    "        cols = np.array(range(old_cell[1], new_cell[1] + 1))\n",
    "        fin = set([tuple(x) for x in self.data.finish_line])\n",
    "        row_col_matrix = [(x, y) for x in rows for y in cols]\n",
    "        intersect = [x for x in row_col_matrix if x in fin]\n",
    "\n",
    "        return len(intersect) > 0\n",
    "    \n",
    "    def is_out_of_track(self, state, action):\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "\n",
    "        if (new_cell[0] < 0 or new_cell[0] > 99 or new_cell[1] < 0 or new_cell[1] > 99):\n",
    "            return True\n",
    "        else:\n",
    "            return self.data.racetrack[tuple(new_cell)] == -1\n",
    "        \n",
    "    def __init__(self, data, gen):\n",
    "        self.data = data\n",
    "        self.gen = gen\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.data.episode = dict({'S': [], 'A': [], 'R': [None], 'probs': []})\n",
    "        self.step_count = 0\n",
    "\n",
    "    def start(self):\n",
    "        state = np.zeros(4, dtype='int')\n",
    "        state[0] = 99\n",
    "        state[1] = self.select_randomly(self.data.start_line[:,1])\n",
    "        return state\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        self.data.episode['A'].append(action) \n",
    "        reward = -1\n",
    "\n",
    "        if self.is_finish_line_crossed(state, action):\n",
    "            new_state = self.get_new_state(state, action)\n",
    "\n",
    "            self.data.episode['S'].append(new_state)\n",
    "            self.data.episode['R'].append(reward)\n",
    "            self.step_count += 1\n",
    "\n",
    "            return None, new_state\n",
    "        \n",
    "        elif self.is_out_of_track(state, action):\n",
    "            new_state = self.start()\n",
    "        else:\n",
    "            new_state = self.get_new_state(state, action)\n",
    "\n",
    "        self.data.episode['S'].append(new_state)\n",
    "        self.data.episode['R'].append(reward)\n",
    "        self.step_count += 1\n",
    "\n",
    "        return reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def possible_actions(self, velocity):\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        α = [np.array(x) for x in α]\n",
    "        \n",
    "        β = []\n",
    "        for i, x in zip(range(len(α)), α):\n",
    "            new_vel = np.add(velocity, x)\n",
    "            if (new_vel[0] < 5) and (new_vel[0] >= 0) and (new_vel[1] < 5) and (new_vel[1] >= 0) and ~(new_vel == np.array([0, 0])):\n",
    "                β.append(i)\n",
    "        β = np.array(β)\n",
    "\n",
    "        return β\n",
    "    \n",
    "    def map_to_1D(self, action):\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        for i, x in zip(range(len(α)), α):\n",
    "            if action[0]==x[0] and action[1]==x[1]:\n",
    "                return i\n",
    "            \n",
    "    def map_to_2D(self, action):\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        return α[action]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_action(self, state, policy):\n",
    "        return self.map_to_2D(policy(state, self.possible_actions(state[2:4])))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "class Visualizer:\n",
    "    def create_window(self):\n",
    "        self.display = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption('Racetrack')\n",
    "\n",
    "    def setup(self):\n",
    "        self.cell_edge = 9\n",
    "        self.width = 100 * self.cell_edge\n",
    "        self.height = 100 * self.cell_edge\n",
    "        self.create_window()\n",
    "        self.window = True\n",
    "\n",
    "    def close_window(self):\n",
    "        self.window = False\n",
    "        pygame.quit()\n",
    "    \n",
    "    def draw(self, state = np.array([])):\n",
    "        self.display.fill(0)\n",
    "        for i in range(100):\n",
    "            for j in range(100):\n",
    "                if self.data.racetrack[i, j] != -1:\n",
    "                    if self.data.racetrack[i, j] == 0:\n",
    "                        color = (255, 0, 0)\n",
    "                    elif self.data.racetrack[i, j] == 1:\n",
    "                        color = (255, 255, 0)\n",
    "                    elif self.data.racetrack[i, j] == 2:\n",
    "                        color = (0, 255, 0)\n",
    "                    pygame.draw.rect(self.display,color,((j*self.cell_edge,i*self.cell_edge),(self.cell_edge,self.cell_edge)),1)\n",
    "\n",
    "        if len(state) > 0:\n",
    "            pygame.draw.rect(self.display,(0,0,255),((state[1]*self.cell_edge,state[0]*self.cell_edge),(self.cell_edge,self.cell_edge)),1)\n",
    "        \n",
    "        pygame.display.update()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.loop = False\n",
    "                self.close_window()\n",
    "                return 'stop'\n",
    "            elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                self.loop = False\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def visualize_racetrack(self, state = np.array([])):\n",
    "        if self.window == False:\n",
    "            self.setup()\n",
    "        self.loop = True\n",
    "        while self.loop:\n",
    "            ret = self.draw(state)\n",
    "            if ret != None:\n",
    "                return ret\n",
    "            \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.window = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Monte_Carlo_Control:\n",
    "\n",
    "    def evaluate_target_policy(self):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew != None:\n",
    "            action = agent.get_action(state, self.data.target_policy_action)\n",
    "            rew, state = env.step(state, action)\n",
    "        \n",
    "        self.data.rewards.append(sum(self.data.episode['R'][1:]))\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        ax, fig = plt.subplots(figsize=(30,15))\n",
    "        x = np.arange(1,len(self.data.rewards)+1)\n",
    "        plt.plot(x*10, self.data.rewards, linewidth=0.5, color = '#BB8FCE')\n",
    "        plt.xlabel('Episode number', size = 20)\n",
    "        plt.ylabel('Reward',size = 20)\n",
    "        plt.title('Plot of Reward vs Episode Number',size=20)\n",
    "        plt.xticks(size=20)\n",
    "        plt.yticks(size=20)\n",
    "        plt.savefig('RewardGraph.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_your_work(self):\n",
    "        self.data.save_Q_vals()\n",
    "        self.data.save_C_vals()\n",
    "        self.data.save_π()\n",
    "        self.data.save_rewards()\n",
    "\n",
    "    def determine_probility_behavior(self, state, action, possible_actions):\n",
    "        best_action = self.data.π[tuple(state)]\n",
    "        num_actions = len(possible_actions)\n",
    "\n",
    "        if best_action in possible_actions:\n",
    "            if action == best_action:\n",
    "                prob = 1 - self.data.ε + self.data.ε/num_actions\n",
    "            else:\n",
    "                prob = self.data.ε/num_actions\n",
    "        else:\n",
    "            prob = 1/num_actions\n",
    "\n",
    "        self.data.episode['probs'].append(prob)\n",
    "\n",
    "    def generate_target_policy_action(self, state, possible_actions):\n",
    "        if self.data.π[tuple(state)] in possible_actions:\n",
    "            action = self.data.π[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def generate_behavior_policy_action(self, state, possible_actions):\n",
    "        if np.random.rand() > self.data.ε and self.data.π[tuple(state)] in possible_actions:\n",
    "            action = self.data.π[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "        \n",
    "        self.determine_probility_behavior(state, action, possible_actions)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Initialize, for all s ∈ S, a ∈ A(s):\n",
    "            data.Q(s, a) ← arbitrary (done in Data)\n",
    "            data.C(s, a) ← 0 (done in Data)\n",
    "            π(s) ← argmax_a Q(s,a) \n",
    "            (with ties broken consistently) \n",
    "            (some consistent approach needs to be followed))\n",
    "        '''\n",
    "        self.data = data\n",
    "        for i in range(100):\n",
    "            for j in range(100):\n",
    "                if self.data.racetrack[i,j]!=-1:\n",
    "                    for k in range(5):\n",
    "                        for l in range(5):\n",
    "                            self.data.π[i,j,k,l] = np.argmax(self.data.Q_vals[i,j,k,l])\n",
    "    \n",
    "    def control(self,env,agent):\n",
    "        '''\n",
    "        Performs MC control using episode list [ S0 , A0 , R1, . . . , ST −1 , AT −1, RT , ST ]\n",
    "        G ← 0\n",
    "        W ← 1\n",
    "        For t = T − 1, T − 2, . . . down to 0:\n",
    "            G ← γ*G + R_t+1\n",
    "            C(St, At ) ← C(St,At ) + W\n",
    "            Q(St, At ) ← Q(St,At) + (W/C(St,At))*[G − Q(St,At )]\n",
    "            π(St) ← argmax_a Q(St,a) (with ties broken consistently)\n",
    "            If At != π(St) then exit For loop\n",
    "            W ← W * (1/b(At|St))        \n",
    "        '''\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_behavioural_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        T = env.step_count\n",
    "    \n",
    "        for t in range(T-1,-1,-1):\n",
    "            G = data.γ * G + self.data.episode['R'][t+1]\n",
    "            S_t = tuple(self.data.episode['S'][t])\n",
    "            A_t = agent.map_to_1D(self.data.episode['A'][t])\n",
    "            \n",
    "            S_list = list(S_t)\n",
    "            S_list.append(A_t)\n",
    "            SA = tuple(S_list)\n",
    "            \n",
    "            self.data.C_vals[SA] += W\n",
    "            self.data.Q_vals[SA] += (W*(G-self.data.Q_vals[SA]))/(self.data.C_vals[SA])           \n",
    "            self.data.π[S_t] = np.argmax(self.data.Q_vals[S_t])\n",
    "            if A_t!=self.data.π[S_t]:\n",
    "                break\n",
    "            W /= self.data.episode['probs'][t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/solving-racetrack-in-reinforcement-learning-using-monte-carlo-control-bdee2aa4f04e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "gen = Generator()\n",
    "env = Environment(data,gen)\n",
    "mcc = Monte_Carlo_Control(data)\n",
    "vis = Visualizer(data)\n",
    "agent = Agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
